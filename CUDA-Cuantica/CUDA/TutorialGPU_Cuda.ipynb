{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TutorialGPU_Cuda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHzXLbTIEHpm",
        "outputId": "adb08ef7-80a0-4c9b-811e-3ab83c2e65ad"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  7 20:21:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv2oYWS17j8m",
        "outputId": "2c2c046d-6fca-4a8d-c562-b97eaf76c14b"
      },
      "source": [
        "%cd /usr/local/cuda/samples/1_Utilities/deviceQuery/\n",
        "!make\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/samples/1_Utilities/deviceQuery\n",
            "/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "/usr/local/cuda-11.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o \n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla K80\"\n",
            "  CUDA Driver Version / Runtime Version          11.2 / 11.1\n",
            "  CUDA Capability Major/Minor version number:    3.7\n",
            "  Total amount of global memory:                 11441 MBytes (11996954624 bytes)\n",
            "  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores\n",
            "  GPU Max Clock rate:                            824 MHz (0.82 GHz)\n",
            "  Memory Clock rate:                             2505 Mhz\n",
            "  Memory Bus Width:                              384-bit\n",
            "  L2 Cache Size:                                 1572864 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        114688 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            No\n",
            "  Supports Cooperative Kernel Launch:            No\n",
            "  Supports MultiDevice Co-op Kernel Launch:      No\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.2, CUDA Runtime Version = 11.1, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tiuxb8ZcMg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c1e18c-ff17-4ef6-e452-414cdc1c3f35"
      },
      "source": [
        "%cd /content/\n",
        "!mkdir workcuda\n",
        "!cd workcuda/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH1Xtcend3Bf"
      },
      "source": [
        "Crear un fichero y escribir el contenindo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAyqNfAd95Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebcf5615-9616-4705-c998-dbc90de0593d"
      },
      "source": [
        "%%writefile suma1d.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    \n",
        "  // host copies of variables a, b & c\n",
        "  int a, b, c;\n",
        "  \n",
        "  // device copies of variables a, b & c  \n",
        "  int *d_a, *d_b, *d_c;\n",
        "  \n",
        "  int size = sizeof(int);\n",
        "\n",
        "  // Allocate space for device copies of a, b, c\n",
        "  cudaMalloc((void **)&d_a, size);\n",
        "  cudaMalloc((void **)&d_b, size);\n",
        "  cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "  // Setup input values  \n",
        "  c = 0;\n",
        "  a = 3;\n",
        "  b = 5;\n",
        "\n",
        "  // Copy inputs to device\n",
        "  cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Launch add() kernel on GPU\n",
        "  add<<<1,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "  // Copy result back to host\n",
        "  cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) \n",
        "    printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  \n",
        "  printf(\"result is %d\\n\",c);\n",
        "\n",
        "  // Cleanup\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma1d.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5lNkDHfZGl"
      },
      "source": [
        "Compilar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Wx8K-ffJOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d1c17a-f31b-480d-cb10-29b8d740268c"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma1d.cu -o suma1d -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsPg1BeIfWeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb60605a-8383-429a-875c-bdfe3abdbd7e"
      },
      "source": [
        "!./suma1d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjD-FD6fjHM"
      },
      "source": [
        "Comprobar el perfil de ejecución"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpSV8mdMfomL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37f7df9-f796-49a0-a255-fe2f7e0be7b6"
      },
      "source": [
        "!nvprof ./suma1d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==249== NVPROF is profiling process 249, command: ./suma1d\n",
            "==249== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "result is 8\n",
            "==249== Profiling application: ./suma1d\n",
            "==249== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   37.58%  3.7760us         2  1.8880us  1.5360us  2.2400us  [CUDA memcpy HtoD]\n",
            "                   36.94%  3.7120us         1  3.7120us  3.7120us  3.7120us  add(int*, int*, int*)\n",
            "                   25.48%  2.5600us         1  2.5600us  2.5600us  2.5600us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.59%  248.11ms         3  82.702ms  2.8100us  248.10ms  cudaMalloc\n",
            "                    0.22%  557.47us         1  557.47us  557.47us  557.47us  cuDeviceTotalMem\n",
            "                    0.07%  186.28us       101  1.8440us     142ns  75.285us  cuDeviceGetAttribute\n",
            "                    0.06%  156.75us         3  52.250us  5.4870us  137.77us  cudaFree\n",
            "                    0.02%  54.267us         3  18.089us  12.015us  25.496us  cudaMemcpy\n",
            "                    0.01%  25.722us         1  25.722us  25.722us  25.722us  cudaLaunchKernel\n",
            "                    0.01%  25.170us         1  25.170us  25.170us  25.170us  cuDeviceGetName\n",
            "                    0.00%  7.7300us         1  7.7300us  7.7300us  7.7300us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7790us         3     593ns     238ns  1.0100us  cuDeviceGetCount\n",
            "                    0.00%  1.5520us         2     776ns     289ns  1.2630us  cuDeviceGet\n",
            "                    0.00%     363ns         1     363ns     363ns     363ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syklEBArgtio"
      },
      "source": [
        "# **PARALELIZAR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA7Y68Jfb46T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1bc55c-32ed-4b94-8cf6-1e98313d6949"
      },
      "source": [
        "!rm suma2dvector.cu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'suma2dvector.cu': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FqMcRGNHCqu"
      },
      "source": [
        "# Lanzando N Bloques de 1 Thread"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDULMwPfgxuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1c11d4-211e-416b-e31d-01edb889e641"
      },
      "source": [
        "%%writefile suma2dvector.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "//*c = *a + *b;\n",
        "c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 2048\n",
        "#define NUMBLOCKS 65538\n",
        "\n",
        "int main(void) {\n",
        "    \n",
        "  int *a, *b, *c;        // host copies of variables a, b & c\n",
        "  int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "  int size = N * sizeof(int);\n",
        "\n",
        "  // Allocate space for host copies of a, b, c   Setup input values  \n",
        "  a =  (int *) malloc(size); \n",
        "  b =  (int *) malloc(size); \n",
        "  c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "  //   Setup input values  \n",
        "\n",
        "  for( int i = 0; i < N; i++ ){\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "  // Allocate space for device copies of a, b, c\n",
        "  cudaMalloc((void **)&d_a, size);\n",
        "  cudaMalloc((void **)&d_b, size);\n",
        "  cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "  // Copy inputs to device\n",
        "  cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Launch add() kernel on GPU  Se lanzan N bloques de 1 Thread.\n",
        "  add<<<NUMBLOCKS,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "  // Copy result back to host\n",
        "  cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "  printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "  printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "  printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "  printf(\" valor a[2] es %d\\n\",a[2]);\n",
        "  printf(\" valor b[2] es %d\\n\",b[2]);\n",
        "  printf(\"resultado c[2] es %d\\n\",c[2]);\n",
        "\n",
        "  bool res = 1;\n",
        "  for(int i  = 0; i < N; i++)\n",
        "    if(c[i] != N)\n",
        "      res = 0;\n",
        "\n",
        "  printf(\"Todos los valores de c valen lo mismo (1 true, 0 false): %d\\n\",res);\n",
        "\n",
        "  // Cleanup\n",
        "\n",
        "  free(a); free(b);free(c); \n",
        "\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting suma2dvector.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w1xk4RFDZ9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6759f19a-5ba1-4849-c569-45866c8d827f"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvector.cu -o suma2dvector -lcudadevrt\n",
        "!./suma2dvector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[0] es 0\n",
            " valor b[0] es 2048\n",
            "resultado c[0] es 2048\n",
            " valor a[2] es 2\n",
            " valor b[2] es 2046\n",
            "resultado c[2] es 2048\n",
            "Todos los valores de c valen lo mismo (1 true, 0 false): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lUTdjJuFTnR"
      },
      "source": [
        "Comprobar el perfil de ejecución y compare el resultado con el caso anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPBo6tWUFWyb",
        "outputId": "944ea332-b7d7-41d6-b627-2faa36028f2e"
      },
      "source": [
        "!nvprof ./suma2dvector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==297== NVPROF is profiling process 297, command: ./suma2dvector\n",
            "==297== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[0] es 0\n",
            " valor b[0] es 65536\n",
            "resultado c[0] es 65536\n",
            " valor a[2] es 2\n",
            " valor b[2] es 65534\n",
            "resultado c[2] es 65536\n",
            "Todos los valores de c valen lo mismo (1 true, 0 false): 1\n",
            "==297== Profiling application: ./suma2dvector\n",
            "==297== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   70.55%  313.25us         1  313.25us  313.25us  313.25us  add(int*, int*, int*)\n",
            "                   21.45%  95.232us         2  47.616us  38.432us  56.800us  [CUDA memcpy HtoD]\n",
            "                    8.01%  35.552us         1  35.552us  35.552us  35.552us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.24%  195.93ms         3  65.311ms  2.1750us  195.91ms  cudaMalloc\n",
            "                    0.30%  596.91us         3  198.97us  53.697us  456.90us  cudaMemcpy\n",
            "                    0.25%  490.02us         1  490.02us  490.02us  490.02us  cuDeviceTotalMem\n",
            "                    0.09%  186.64us       101  1.8470us     152ns  74.992us  cuDeviceGetAttribute\n",
            "                    0.08%  160.47us         3  53.489us  12.576us  132.18us  cudaFree\n",
            "                    0.02%  37.870us         1  37.870us  37.870us  37.870us  cudaLaunchKernel\n",
            "                    0.01%  26.353us         1  26.353us  26.353us  26.353us  cuDeviceGetName\n",
            "                    0.00%  5.6680us         1  5.6680us  5.6680us  5.6680us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.9200us         3     640ns     182ns  1.0050us  cuDeviceGetCount\n",
            "                    0.00%  1.5310us         2     765ns     398ns  1.1330us  cuDeviceGet\n",
            "                    0.00%     271ns         1     271ns     271ns     271ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0FbrAaUHUQI"
      },
      "source": [
        "# Lanzando 1 Bloque de N Threads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ujd3TwHcE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6eb6f09-13c2-48b0-9c58-7c9ccb8cf734"
      },
      "source": [
        "%%writefile suma2dvectorNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "c[threadIdx.x] = a[threadIdx.x] + b[threadIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 2048\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan 1 bloques de N Threads.\n",
        "add<<<1,N>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[10] es %d\\n\",a[10]);\n",
        "printf(\" valor b[10] es %d\\n\",b[10]);\n",
        "printf(\"resultado c[10] es %d\\n\",c[10]);\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "\n",
        "\n",
        "bool res = 1;\n",
        "int well = 0;\n",
        "  for(int i  = 0; i < N; i++)\n",
        "    if(c[i] == N){\n",
        "      well +=1;\n",
        "    }\n",
        "\n",
        "      \n",
        "res = well == N;\n",
        "\n",
        "  printf(\"Todos los valores de c valen lo mismo (1 true, 0 false): %d\\n\",res);\n",
        "  printf(\"Numero de elementos sumados correctamente: %d\\n\",well);\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvectorNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW4YFVPkIYHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb8888d-783d-4e4c-9fe3-bfa35f07ab0f"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNthreads.cu -o suma2dvectorNthreads -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHbZLnqfJCVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ad74a4-194f-4616-f310-b08370eff719"
      },
      "source": [
        "!./suma2dvectorNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " valor a[10] es 10\n",
            " valor b[10] es 2038\n",
            "resultado c[10] es 0\n",
            " valor a[0] es 0\n",
            " valor b[0] es 2048\n",
            "resultado c[0] es 0\n",
            "Todos los valores de c valen lo mismo (1 true, 0 false): 0\n",
            "Numero de elementos sumados correctamente: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubd_5nLCImd0"
      },
      "source": [
        "Comprobar el perfil de ejecución y compare el resultado con los casos anteriores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhGdz2EMw5W3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620139c6-67ad-404b-9b29-b25c3655471a"
      },
      "source": [
        "!nvprof ./suma2dvectorNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==344== NVPROF is profiling process 344, command: ./suma2dvectorNthreads\n",
            "==344== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 2038\n",
            "resultado c[10] es 0\n",
            " valor a[0] es 0\n",
            " valor b[0] es 2048\n",
            "resultado c[0] es 0\n",
            "Todos los valores de c valen lo mismo (1 true, 0 false): 0\n",
            "Numero de elementos sumados correctamente: 0\n",
            "==344== Profiling application: ./suma2dvectorNthreads\n",
            "==344== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   63.69%  6.8480us         2  3.4240us  3.3600us  3.4880us  [CUDA memcpy HtoD]\n",
            "                   36.31%  3.9040us         1  3.9040us  3.9040us  3.9040us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.54%  202.26ms         3  67.421ms  2.4890us  202.26ms  cudaMalloc\n",
            "                    0.25%  498.93us         1  498.93us  498.93us  498.93us  cuDeviceTotalMem\n",
            "                    0.09%  182.67us       101  1.8080us     143ns  74.404us  cuDeviceGetAttribute\n",
            "                    0.08%  154.00us         3  51.332us  5.8340us  133.05us  cudaFree\n",
            "                    0.03%  60.979us         3  20.326us  14.209us  26.236us  cudaMemcpy\n",
            "                    0.01%  29.125us         1  29.125us  29.125us  29.125us  cuDeviceGetName\n",
            "                    0.00%  7.4360us         1  7.4360us  7.4360us  7.4360us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.6210us         3     873ns     258ns  1.3340us  cuDeviceGetCount\n",
            "                    0.00%  1.7500us         2     875ns     454ns  1.2960us  cuDeviceGet\n",
            "                    0.00%     699ns         1     699ns     699ns     699ns  cudaLaunchKernel\n",
            "                    0.00%     293ns         1     293ns     293ns     293ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-02ilKuw7Mp"
      },
      "source": [
        "# Preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzr6eOyUH3VR"
      },
      "source": [
        "* Pruebe a lanzar diferente número de Threads ( con un solo 1 bloque)\n",
        "¿Cual son los valores máximos y mínimos de número de theads por bloque en esta GPU?\n",
        "\n",
        "Pruebe a lanzar diferente número de bloques ( con un solo thread) ¿Cual son los valores máximos y mínimos de número de bloques en esta GPU?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX_QYtIPxjBb"
      },
      "source": [
        "#Lanzando NUMBLOQ Bloques de NUMTHREADS Threads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTzYtat43hU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5396160-dba0-47a6-dd9b-b0324c4a0744"
      },
      "source": [
        "!rm suma2dvectorNBloqxNthreads.cu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'suma2dvectorNBloqxNthreads.cu': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spb5jhxKyGz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d93d40-32e9-4041-b61c-415514917180"
      },
      "source": [
        "\n",
        "%%writefile suma2dvectorNBloqxNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        " c[index] = a[index] + b[index]; \n",
        "}\n",
        "\n",
        "\n",
        "#define N (1024*1024 -1)\n",
        "#define THREADS_PER_BLOCK 512 \n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N*sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan NuBloq=N/THREADS_PER_BLOCK bloques de \n",
        "// THREADS_PER_BLOCK Threads.\n",
        "add<<<(N + THREADS_PER_BLOCK - 1)/THREADS_PER_BLOCK,THREADS_PER_BLOCK>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "\n",
        "bool res = 1;\n",
        "int well = 0;\n",
        "  for(int i  = 0; i < N; i++)\n",
        "    if(c[i] == N){\n",
        "      well +=1;\n",
        "    }\n",
        "\n",
        "      \n",
        "res = well == N;\n",
        "\n",
        "  printf(\"Todos los valores de c valen lo mismo (1 true, 0 false): %d\\n\",res);\n",
        "  printf(\"Numero de elementos sumados correctamente: %d\\n\",well);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvectorNBloqxNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxIvXWKCzBxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a03cec5-638e-4dd4-c844-849110080c37"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "Todos los valores de c valen lo mismo (1 true, 0 false): 1\n",
            "Numero de elementos sumados correctamente: 1048575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H08bppmr2UKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aec84f8-d45c-40d8-c91b-bb4f91a1866c"
      },
      "source": [
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==392== NVPROF is profiling process 392, command: ./suma2dvectorNBloqxNthreads\n",
            "==392== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Todos los valores de c valen lo mismo (1 true, 0 false): 1\n",
            "Numero de elementos sumados correctamente: 1048575\n",
            "==392== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==392== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.81%  1.2620ms         2  631.01us  594.78us  667.23us  [CUDA memcpy HtoD]\n",
            "                   28.70%  550.40us         1  550.40us  550.40us  550.40us  [CUDA memcpy DtoH]\n",
            "                    5.49%  105.38us         1  105.38us  105.38us  105.38us  add(int*, int*, int*)\n",
            "      API calls:   97.79%  201.08ms         3  67.026ms  142.82us  200.74ms  cudaMalloc\n",
            "                    1.18%  2.4285ms         3  809.49us  749.42us  912.26us  cudaMemcpy\n",
            "                    0.64%  1.3203ms         3  440.08us  149.00us  594.12us  cudaFree\n",
            "                    0.24%  501.75us         1  501.75us  501.75us  501.75us  cuDeviceTotalMem\n",
            "                    0.10%  208.71us       101  2.0660us     153ns  99.489us  cuDeviceGetAttribute\n",
            "                    0.02%  36.999us         1  36.999us  36.999us  36.999us  cudaLaunchKernel\n",
            "                    0.01%  30.122us         1  30.122us  30.122us  30.122us  cuDeviceGetName\n",
            "                    0.00%  5.6040us         1  5.6040us  5.6040us  5.6040us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1830us         3     727ns     256ns     995ns  cuDeviceGetCount\n",
            "                    0.00%  1.7520us         2     876ns     490ns  1.2620us  cuDeviceGet\n",
            "                    0.00%     325ns         1     325ns     325ns     325ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Plee2mF3HSI"
      },
      "source": [
        "Pregunta ¿Qué sucede si incrementa o disminuye el valor de N?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-KF2vJwKiqj"
      },
      "source": [
        "# SUMA Matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-UzvQnwUqlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c8d3d9-c568-4178-c7f7-1172549c2809"
      },
      "source": [
        "\n",
        "%%writefile sumaMatrices.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c, int r, int col) {\n",
        "    int i = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "    int j = (blockIdx.y * blockDim.y) + threadIdx.y;\n",
        "    int index = col*i + j;\n",
        "\n",
        "    if (i < r && j < col)\n",
        "      c[index] = a[index] + b[index]; \n",
        "}\n",
        "\n",
        "#define R 1024\n",
        "#define C 1014\n",
        "#define THREADS_PER_BLOCK 8\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = R*C*sizeof(int);\n",
        "// int N = R*C;\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < R; i++ ){\n",
        "    for(int j = 0; j < C; j++){\n",
        "        a[i*C + j] = 2;\n",
        "        b[i*C + j] = 3;\n",
        "    }    \n",
        "}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **) &d_a, size);\n",
        "cudaMalloc((void **) &d_b, size);\n",
        "cudaMalloc((void **) &d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "dim3 dimGrid((R + THREADS_PER_BLOCK - 1)/THREADS_PER_BLOCK, (C + THREADS_PER_BLOCK -1)/THREADS_PER_BLOCK, 1);\n",
        "\n",
        "dim3 dimBlock(THREADS_PER_BLOCK, THREADS_PER_BLOCK, 1);\n",
        "\n",
        "\n",
        "\n",
        "add<<<dimGrid,dimBlock>>>(d_a, d_b, d_c,R,C); \n",
        "\n",
        "cudaDeviceSynchronize();\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "\n",
        "\n",
        "  int errors = 0;\n",
        "  for(int i = 0; i < R; i++){\n",
        "      for(int j = 0; j < 1; j++){\n",
        "          if (c[i*C + j] != 5)\n",
        "            errors++;\n",
        "          //printf(\"%d\", d_c[i*C + j]);\n",
        "      }\n",
        "  }\n",
        "\n",
        "\n",
        "  printf(\"Número de errores: %d\\n\",errors);\n",
        "\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sumaMatrices.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKrRXP-1N8YS",
        "outputId": "c76f0ac2-d96e-4b9e-cee4-e122ca1750d5"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true sumaMatrices.cu -o sumaMatrices -lcudadevrt\n",
        "!./sumaMatrices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "Número de errores: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f7pqVme_Htb",
        "outputId": "233e83bd-7a08-442c-b717-f93c6b1413c2"
      },
      "source": [
        "!nvprof ./sumaMatrices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==439== NVPROF is profiling process 439, command: ./sumaMatrices\n",
            "==439== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Número de errores: 0\n",
            "==439== Profiling application: ./sumaMatrices\n",
            "==439== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   53.27%  1.6100ms         1  1.6100ms  1.6100ms  1.6100ms  [CUDA memcpy DtoH]\n",
            "                   36.96%  1.1170ms         2  558.48us  536.45us  580.51us  [CUDA memcpy HtoD]\n",
            "                    9.77%  295.20us         1  295.20us  295.20us  295.20us  add(int*, int*, int*, int, int)\n",
            "      API calls:   96.84%  199.85ms         3  66.618ms  123.89us  199.55ms  cudaMalloc\n",
            "                    1.96%  4.0386ms         3  1.3462ms  642.32us  2.7361ms  cudaMemcpy\n",
            "                    0.63%  1.3001ms         3  433.35us  139.38us  584.25us  cudaFree\n",
            "                    0.23%  478.70us         1  478.70us  478.70us  478.70us  cuDeviceTotalMem\n",
            "                    0.21%  435.25us         1  435.25us  435.25us  435.25us  cudaDeviceSynchronize\n",
            "                    0.10%  202.62us       101  2.0060us     150ns  101.23us  cuDeviceGetAttribute\n",
            "                    0.01%  26.895us         1  26.895us  26.895us  26.895us  cudaLaunchKernel\n",
            "                    0.01%  25.039us         1  25.039us  25.039us  25.039us  cuDeviceGetName\n",
            "                    0.00%  6.2000us         1  6.2000us  6.2000us  6.2000us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.9070us         3     635ns     233ns     877ns  cuDeviceGetCount\n",
            "                    0.00%  1.5380us         2     769ns     277ns  1.2610us  cuDeviceGet\n",
            "                    0.00%     295ns         1     295ns     295ns     295ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-22fDEaLSn",
        "outputId": "0771e185-5d56-4c7e-ab73-77a1c725e52b"
      },
      "source": [
        "!sed -i '/#define R/c\\#define R 512' sumaMatrices.cu\n",
        "!sed -i '/#define C/c\\#define C 1024' sumaMatrices.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true sumaMatrices.cu -o sumaMatrices_changedim -lcudadevrt\n",
        "!nvprof ./sumaMatrices_changedim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==708== NVPROF is profiling process 708, command: ./sumaMatrices_changedim\n",
            "==708== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Número de errores: 0\n",
            "==708== Profiling application: ./sumaMatrices_changedim\n",
            "==708== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   59.12%  569.54us         2  284.77us  282.66us  286.88us  [CUDA memcpy HtoD]\n",
            "                   28.03%  270.02us         1  270.02us  270.02us  270.02us  [CUDA memcpy DtoH]\n",
            "                   12.85%  123.74us         1  123.74us  123.74us  123.74us  add(int*, int*, int*, int, int)\n",
            "      API calls:   98.16%  203.58ms         3  67.859ms  124.40us  203.27ms  cudaMalloc\n",
            "                    0.96%  1.9813ms         3  660.42us  291.06us  1.2887ms  cudaMemcpy\n",
            "                    0.37%  767.02us         3  255.67us  137.30us  318.07us  cudaFree\n",
            "                    0.25%  519.54us         1  519.54us  519.54us  519.54us  cuDeviceTotalMem\n",
            "                    0.13%  262.87us         1  262.87us  262.87us  262.87us  cudaDeviceSynchronize\n",
            "                    0.09%  187.58us       101  1.8570us     152ns  77.437us  cuDeviceGetAttribute\n",
            "                    0.02%  41.571us         1  41.571us  41.571us  41.571us  cuDeviceGetName\n",
            "                    0.02%  35.077us         1  35.077us  35.077us  35.077us  cudaLaunchKernel\n",
            "                    0.01%  20.030us         1  20.030us  20.030us  20.030us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0600us         3     686ns     199ns  1.1440us  cuDeviceGetCount\n",
            "                    0.00%  1.6560us         2     828ns     302ns  1.3540us  cuDeviceGet\n",
            "                    0.00%     317ns         1     317ns     317ns     317ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d66hinKherC4",
        "outputId": "66cd986d-269f-4633-e9de-6f58ed7cdf90"
      },
      "source": [
        "!sed -i '/#define R/c\\#define R 111' sumaMatrices.cu\n",
        "!sed -i '/#define C/c\\#define C 333' sumaMatrices.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true sumaMatrices.cu -o sumaMatrices_changedim_2 -lcudadevrt\n",
        "!nvprof ./sumaMatrices_changedim_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==792== NVPROF is profiling process 792, command: ./sumaMatrices_changedim_2\n",
            "==792== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Número de errores: 0\n",
            "==792== Profiling application: ./sumaMatrices_changedim_2\n",
            "==792== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   58.69%  46.240us         2  23.120us  22.080us  24.160us  [CUDA memcpy HtoD]\n",
            "                   26.97%  21.248us         1  21.248us  21.248us  21.248us  [CUDA memcpy DtoH]\n",
            "                   14.34%  11.296us         1  11.296us  11.296us  11.296us  add(int*, int*, int*, int, int)\n",
            "      API calls:   99.39%  207.92ms         3  69.306ms  4.0150us  207.91ms  cudaMalloc\n",
            "                    0.26%  546.53us         1  546.53us  546.53us  546.53us  cuDeviceTotalMem\n",
            "                    0.12%  243.93us         3  81.309us  52.364us  130.09us  cudaMemcpy\n",
            "                    0.12%  240.88us       101  2.3840us     177ns  87.481us  cuDeviceGetAttribute\n",
            "                    0.07%  144.59us         3  48.196us  3.9960us  120.94us  cudaFree\n",
            "                    0.02%  38.427us         1  38.427us  38.427us  38.427us  cudaLaunchKernel\n",
            "                    0.02%  31.407us         1  31.407us  31.407us  31.407us  cuDeviceGetName\n",
            "                    0.01%  18.388us         1  18.388us  18.388us  18.388us  cudaDeviceSynchronize\n",
            "                    0.00%  8.4000us         1  8.4000us  8.4000us  8.4000us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.7880us         3     929ns     279ns  1.2740us  cuDeviceGetCount\n",
            "                    0.00%  1.8820us         2     941ns     412ns  1.4700us  cuDeviceGet\n",
            "                    0.00%     317ns         1     317ns     317ns     317ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJjBuKx3VYo"
      },
      "source": [
        "#EJERCICIO : realice el mismo procedimiento para la ejecución de Stencil1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uKzTtEeISQw"
      },
      "source": [
        "Compile el siguiente código, explique su funcionamiento y compruebe si la salida es correcta. En caso de que sea necesario realice las modificaciones oportunas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGaG1nrF3hKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1159a5-895b-4fcb-8b92-615e85a59dbd"
      },
      "source": [
        "%%writefile stenciltest.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    //__shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    //int lindex = threadIdx.x + RADIUS;\n",
        " \n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    //temp[lindex] = in[gindex];\n",
        "    //if (threadIdx.x < RADIUS) \n",
        "    //{\n",
        "        //temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        //temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    //__syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += in[gindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting stenciltest.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXahjwTKG4Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2830c274-d129-46b9-8dd0-18a911d3ebee"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest.cu -o ./stenciltest -lcudadevrt\n",
        "!nvprof ./stenciltest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "stenciltest.cu(20): warning: variable \"gindex\" was declared but never referenced\n",
            "\n",
            "stenciltest.cu(37): error: expected a declaration\n",
            "\n",
            "stenciltest.cu(41): error: identifier \"gindex\" is undefined\n",
            "\n",
            "stenciltest.cu(42): error: expected a declaration\n",
            "\n",
            "stenciltest.cu(74): warning: parsing restarts here after previous syntax error\n",
            "\n",
            "stenciltest.cu(77): error: this declaration has no storage class or type specifier\n",
            "\n",
            "stenciltest.cu(77): error: declaration is incompatible with \"cudaError_t cudaFree(void *)\"\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_device_runtime_api.h(145): here\n",
            "\n",
            "stenciltest.cu(77): error: identifier \"d_in\" is undefined\n",
            "\n",
            "stenciltest.cu(78): error: this declaration has no storage class or type specifier\n",
            "\n",
            "stenciltest.cu(78): error: variable \"cudaFree\" has already been defined\n",
            "\n",
            "stenciltest.cu(78): error: identifier \"d_out\" is undefined\n",
            "\n",
            "stenciltest.cu(80): error: expected a declaration\n",
            "\n",
            "stenciltest.cu(81): error: expected a declaration\n",
            "\n",
            "11 errors detected in the compilation of \"stenciltest.cu\".\n",
            "==327== NVPROF is profiling process 327, command: ./stenciltest\n",
            "==327== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==327== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "SUCCESS!\n",
            "==327== Profiling application: ./stenciltest\n",
            "==327== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.47%  8.9920us         1  8.9920us  8.9920us  8.9920us  [CUDA memcpy HtoD]\n",
            "                   34.30%  6.7840us         1  6.7840us  6.7840us  6.7840us  [CUDA memcpy DtoH]\n",
            "                   20.23%  4.0000us         1  4.0000us  4.0000us  4.0000us  stencil_1d(int*, int*)\n",
            "      API calls:   99.33%  211.89ms         2  105.94ms  4.8640us  211.88ms  cudaMalloc\n",
            "                    0.34%  728.11us         1  728.11us  728.11us  728.11us  cuDeviceTotalMem\n",
            "                    0.16%  333.02us       101  3.2970us     260ns  129.07us  cuDeviceGetAttribute\n",
            "                    0.06%  135.61us         2  67.807us  14.165us  121.45us  cudaFree\n",
            "                    0.04%  86.685us         2  43.342us  29.822us  56.863us  cudaMemcpy\n",
            "                    0.03%  71.183us         1  71.183us  71.183us  71.183us  cudaLaunchKernel\n",
            "                    0.03%  65.514us         1  65.514us  65.514us  65.514us  cuDeviceGetName\n",
            "                    0.00%  5.4780us         1  5.4780us  5.4780us  5.4780us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.2960us         3  1.0980us     372ns  1.6620us  cuDeviceGetCount\n",
            "                    0.00%  1.9310us         2     965ns     599ns  1.3320us  cuDeviceGet\n",
            "                    0.00%     528ns         1     528ns     528ns     528ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t7fl5s2IIMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8dec9c-1f03-4497-bd1d-f6de02b55689"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==169== NVPROF is profiling process 169, command: ./stenciltest\n",
            "==169== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==169== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "SUCCESS!\n",
            "==169== Profiling application: ./stenciltest\n",
            "==169== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.40%  8.9920us         1  8.9920us  8.9920us  8.9920us  [CUDA memcpy HtoD]\n",
            "                   33.93%  6.7200us         1  6.7200us  6.7200us  6.7200us  [CUDA memcpy DtoH]\n",
            "                   20.68%  4.0960us         1  4.0960us  4.0960us  4.0960us  stencil_1d(int*, int*)\n",
            "      API calls:   99.53%  255.49ms         2  127.75ms  5.3530us  255.49ms  cudaMalloc\n",
            "                    0.19%  490.76us         1  490.76us  490.76us  490.76us  cuDeviceTotalMem\n",
            "                    0.09%  231.35us         2  115.68us  15.514us  215.84us  cudaFree\n",
            "                    0.07%  180.18us       101  1.7830us     141ns  74.181us  cuDeviceGetAttribute\n",
            "                    0.05%  135.75us         2  67.873us  56.123us  79.624us  cudaMemcpy\n",
            "                    0.05%  125.11us         1  125.11us  125.11us  125.11us  cudaLaunchKernel\n",
            "                    0.01%  34.863us         1  34.863us  34.863us  34.863us  cuDeviceGetName\n",
            "                    0.00%  5.4320us         1  5.4320us  5.4320us  5.4320us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1110us         3     703ns     184ns  1.0000us  cuDeviceGetCount\n",
            "                    0.00%  1.9590us         2     979ns     546ns  1.4130us  cuDeviceGet\n",
            "                    0.00%     300ns         1     300ns     300ns     300ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU0BHCiQJIEd"
      },
      "source": [
        "#EJERCICIO : compile el ejemplo de multiplicación de matrices \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq9bEvZ6JUpq"
      },
      "source": [
        "%cd /usr/local/cuda/samples\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR1tfxYzJeVc"
      },
      "source": [
        "%cd  0_Simple/matrixMul/\n",
        "%ls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m485XaWJJ43M"
      },
      "source": [
        "%pwd\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2AAxHSoKX_K"
      },
      "source": [
        "!cat matrixMul.cu\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}